{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8def34-fade-43cc-b2c5-605a8bc5c857",
   "metadata": {
    "id": "7f8def34-fade-43cc-b2c5-605a8bc5c857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# %load_ext tensorboard\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from torchvision.models.resnet import *\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "RWcAzFTLwFAR",
   "metadata": {
    "id": "RWcAzFTLwFAR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive/', force_remount=True)\\nos.chdir('drive/MyDrive/Colab Notebooks/AICORE/FAISS')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)\n",
    "os.chdir('drive/MyDrive/Colab Notebooks/AICORE/FAISS')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b15a12-db33-4fef-bbed-28a70b7f7d6f",
   "metadata": {
    "id": "04b15a12-db33-4fef-bbed-28a70b7f7d6f"
   },
   "outputs": [],
   "source": [
    "## start the tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b3045d-c421-414c-aaec-55994ef6e380",
   "metadata": {
    "id": "30b3045d-c421-414c-aaec-55994ef6e380"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3b9eb5b670ebd78d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3b9eb5b670ebd78d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {logs_base_dir}  --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25744888",
   "metadata": {
    "id": "25744888"
   },
   "outputs": [],
   "source": [
    "#model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d12edf-672b-459b-9e83-9943f283a7ad",
   "metadata": {
    "id": "41d12edf-672b-459b-9e83-9943f283a7ad"
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6010f360-bb71-4b90-8ba8-d1d074259cd8",
   "metadata": {
    "id": "6010f360-bb71-4b90-8ba8-d1d074259cd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/facebook-marketplaces-recommendation-ranking-system/Practicals'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rjsZNuz7JOVG",
   "metadata": {
    "id": "rjsZNuz7JOVG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\n\\ndef remove_ds_store(root_dir):\\n    for root, _, files in os.walk(root_dir):\\n        for file in files:\\n            if file == \".DS_Store\":\\n                file_path = os.path.join(root, file)\\n                os.remove(file_path)\\n                print(f\"Removed: {file_path}\")\\n\\n# Replace \\'path_to_your_directory\\' with the actual path to the directory you want to clean\\ndirectory_to_clean = \\'aicore_images_split\\'\\nremove_ds_store(directory_to_clean)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "\n",
    "def remove_ds_store(root_dir):\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file == \".DS_Store\":\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Removed: {file_path}\")\n",
    "\n",
    "# Replace 'path_to_your_directory' with the actual path to the directory you want to clean\n",
    "directory_to_clean = 'aicore_images_split'\n",
    "remove_ds_store(directory_to_clean)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eDcucygO_ezJ",
   "metadata": {
    "id": "eDcucygO_ezJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom PIL import Image\\n\\ndef resize_and_convert_to_rgb(image_path, target_size=(64, 64)):\\n    try:\\n        image = Image.open(image_path)\\n        image = image.resize(target_size)\\n\\n        if image.mode != \\'RGB\\':\\n            image = image.convert(\\'RGB\\')\\n\\n        image.save(image_path)\\n        #print(f\"Processed: {image_path}\")\\n    except Exception as e:\\n        print(f\"Error processing {image_path}: {e}\")\\n\\ndef process_subdirectories(root_dir):\\n    for root, _, files in os.walk(root_dir):\\n        for file in files:\\n            if file.lower().endswith((\\'.jpg\\', \\'.jpeg\\', \\'.png\\', \\'.bmp\\', \\'.gif\\')):\\n                image_path = os.path.join(root, file)\\n\\n                resize_and_convert_to_rgb(image_path)\\n\\nroot_directory = \"aicore_images_split\"\\nprocess_subdirectories(root_directory)\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pre process images for speed\n",
    "'''\n",
    "from PIL import Image\n",
    "\n",
    "def resize_and_convert_to_rgb(image_path, target_size=(64, 64)):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(target_size)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        image.save(image_path)\n",
    "        #print(f\"Processed: {image_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "def process_subdirectories(root_dir):\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
    "                image_path = os.path.join(root, file)\n",
    "\n",
    "                resize_and_convert_to_rgb(image_path)\n",
    "\n",
    "root_directory = \"aicore_images_split\"\n",
    "process_subdirectories(root_directory)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb9734df-f22b-460a-9c87-142a45123c65",
   "metadata": {
    "id": "eb9734df-f22b-460a-9c87-142a45123c65"
   },
   "outputs": [],
   "source": [
    "## Creating a train dataset class\n",
    "class ItemsTrainDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.examples = self._load_examples()\n",
    "        #self.resize = transforms.Resize(64)\n",
    "        self.randomhflip = transforms.RandomHorizontalFlip()\n",
    "        #self.randomrot = transforms.RandomRotation(degrees=30)\n",
    "        self.pil_to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        #self.resize = transforms.Resize((225,225))\n",
    "        #self.rgbify = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n",
    "\n",
    "    def _load_examples(self):\n",
    "        class_names = os.listdir('aicore_images_split/train')\n",
    "        class_encoder = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "        class_decoder = {idx: class_name for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "        examples_list = []\n",
    "        for cl_name in class_names:\n",
    "            example_fp = os.listdir(os.path.join('aicore_images_split/train',cl_name))\n",
    "            example_fp = [os.path.join('aicore_images_split/train', cl_name, img_name ) for img_name in example_fp]\n",
    "            example = [(img_name, class_encoder[cl_name]) for img_name in example_fp]\n",
    "            examples_list.extend(example)\n",
    "\n",
    "        return examples_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_fp, img_class = self.examples[idx]\n",
    "        img = Image.open(img_fp)\n",
    "        img = self.randomhflip(img)\n",
    "        #img = self.randomrot(img)\n",
    "\n",
    "        #img = self.resize(img)\n",
    "        features = self.pil_to_tensor(img)\n",
    "        features = self.normalize(features)\n",
    "\n",
    "        #features = self.rgbify(features)\n",
    "\n",
    "        return features, img_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "335d1a2c-34e5-4d3a-9be1-2706e4cb014b",
   "metadata": {
    "id": "335d1a2c-34e5-4d3a-9be1-2706e4cb014b"
   },
   "outputs": [],
   "source": [
    "## Creates a validation dataset class\n",
    "class ItemsValDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.examples = self._load_examples()\n",
    "        #self.resize = transforms.Resize(64)\n",
    "        self.pil_to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        #self.resize = transforms.Resize((225,225))\n",
    "        #self.rgbify = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)!=1 else x)\n",
    "\n",
    "    def _load_examples(self):\n",
    "        class_names = os.listdir('aicore_images_split/val')\n",
    "        class_encoder = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "        class_decoder = {idx: class_name for idx, class_name in enumerate(class_names)}\n",
    "        examples_list = []\n",
    "\n",
    "        for cl_name in class_names:\n",
    "            example_fp = os.listdir(os.path.join('aicore_images_split/val',cl_name))\n",
    "            example_fp = [os.path.join('aicore_images_split/val', cl_name, img_name ) for img_name in example_fp]\n",
    "            example = [(img_name, class_encoder[cl_name]) for img_name in example_fp]\n",
    "            examples_list.extend(example)\n",
    "\n",
    "        return examples_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_fp, img_class = self.examples[idx]\n",
    "        img = Image.open(img_fp)\n",
    "\n",
    "        #img = self.resize(img)\n",
    "        features = self.pil_to_tensor(img)\n",
    "        features = self.normalize(features)\n",
    "\n",
    "        return features, img_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5598f647-b593-4dd2-acd8-7040c6ec2f9d",
   "metadata": {
    "id": "5598f647-b593-4dd2-acd8-7040c6ec2f9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8816"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindataset = ItemsTrainDataSet()\n",
    "len(traindataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5754c01e-76ca-452a-8110-6f3b2aa8f9a4",
   "metadata": {
    "id": "5754c01e-76ca-452a-8110-6f3b2aa8f9a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3788"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valdataset = ItemsValDataSet()\n",
    "len(valdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "060b5233-c30a-4fbe-aa11-17ce9f3e7d75",
   "metadata": {
    "id": "060b5233-c30a-4fbe-aa11-17ce9f3e7d75"
   },
   "outputs": [],
   "source": [
    "## Created a classifier based on the RESNET50 pretrained model\n",
    "\n",
    "class ItemClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
    "        self.resnet50.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet50.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        #self.resnet50 = model\n",
    "        self.resnet50.fc = torch.nn.Linear(2048,13)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return F.softmax(self.resnet50(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "wXlOIqEBDPUr",
   "metadata": {
    "id": "wXlOIqEBDPUr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "classifier = ItemClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afd79aea-afa0-4142-b5c5-8e141a7a223c",
   "metadata": {
    "id": "afd79aea-afa0-4142-b5c5-8e141a7a223c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train(model,traindataloader, valdataloader, epochs):\\n    #optimiser = torch.optim.SGD(model.parameters(), lr=0.001)\\n    #optimiser = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\\n    optimiser = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\\n    #optimiser = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\\n    model_path = str(os.path.join(\\'model_evaluation\\', time.strftime(\"%Y%m%d-%H%M%S\")))\\n    os.makedirs(model_path)\\n    os.makedirs(os.path.join(model_path, \\'weights\\'))\\n\\n    global_step = 0\\n\\n    for epoch in range(epochs):\\n        training_loss = 0.0\\n        validation_loss = 0.0\\n        model.to(device)\\n        model.train()\\n        tr_num_correct = 0\\n        tr_num_examples = 0\\n        epoch_combo = \\'epoch\\' + str(epoch)\\n        os.makedirs(os.path.join(model_path, \\'weights\\', epoch_combo))\\n        for inputs, labels in traindataloader:\\n            #labels = labels.unsqueeze(1)\\n            #labels = labels.float()\\n            inputs = inputs.to(device)\\n            labels = labels.to(device)\\n            predictions = model(inputs)\\n            #print(predictions.shape)\\n            #print(labels.shape)\\n            loss = torch.nn.CrossEntropyLoss()\\n            loss = loss(predictions, labels)\\n            loss.backward()\\n            optimiser.step()\\n            model_save_dir = str(os.path.join(model_path, \\'weights\\', epoch_combo, \\'weights.pt\\'))\\n            full_path = str(os.getcwd())\\n            #print(model_save_dir)\\n            #torch.save({\\'epoch\\': epoch,\\n            #    \\'model_state_dict\\': model.state_dict(),\\n            #    \\'optimizer_state_dict\\': optimiser.state_dict()},\\n                  #str(os.path.join(full_path, model_save_dir)))\\n            #           model_save_dir)\\n            torch.save({\\'epoch\\': epoch,\\n                  \\'model_state_dict\\': model.state_dict(),\\n                  \\'optimizer_state_dict\\': optimiser.state_dict()},\\n                  str(os.path.join(full_path, model_save_dir)))\\n\\n            optimiser.zero_grad()\\n            #batch_idx += 1\\n            training_loss += loss.item() * inputs.size(0)\\n            correct = torch.eq(torch.max(F.softmax(predictions, dim=1), dim=1)[1], labels)\\n            tr_num_correct += torch.sum(correct).item()\\n            tr_num_examples += correct.shape[0]\\n        training_loss /= len(traindataloader.dataset)\\n        training_accuracy = tr_num_correct / tr_num_examples\\n        ## add training performance to tensorboard\\n        writer.add_scalar(\\'Training Loss\\', training_loss, global_step)\\n        writer.add_scalar(\\'Training Accuracy\\', training_accuracy, global_step)\\n\\n        model.eval()\\n        val_num_correct = 0\\n        val_num_examples = 0\\n        for inputs, labels in valdataloader:\\n            #labels = labels.unsqueeze(1)\\n            #labels = labels.float()\\n            inputs = inputs.to(device)\\n            labels = labels.to(device)\\n            predictions = model(inputs)\\n            loss = torch.nn.CrossEntropyLoss()\\n            loss = loss(predictions, labels)\\n            validation_loss += loss.item() * inputs.size(0)\\n            correct = torch.eq(torch.max(F.softmax(predictions, dim =1), dim=1)[1], labels)\\n            val_num_correct += torch.sum(correct).item()\\n            val_num_examples += correct.shape[0]\\n        validation_loss /= len(valdataloader.dataset)\\n        validation_accuracy = val_num_correct / val_num_examples\\n        ## add validation performance to tensorboard\\n        writer.add_scalar(\\'Validation Loss\\', validation_loss, global_step)\\n        writer.add_scalar(\\'Validation Accuracy\\', validation_accuracy, global_step)\\n        perf_dict = {}\\n        perf_dict[epoch] = {\\'training_loss\\': training_loss,\\n                            \\'val_loss\\': validation_loss,\\n                            \\'training_accuracy\\': tr_num_correct / tr_num_examples,\\n                            \\'val_accuracy\\': val_num_correct / val_num_examples}\\n\\n\\n        print(\\'Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, train_accuracy = {:.2f},val_accuracy = {:.2f} \\'.format(epoch, training_loss, validation_loss, tr_num_correct / tr_num_examples,\\n                                                                                                                             val_num_correct / val_num_examples))\\n        global_step += 1\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def train(model,traindataloader, valdataloader, epochs):\n",
    "    #optimiser = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    #optimiser = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    #optimiser = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "    model_path = str(os.path.join('model_evaluation', time.strftime(\"%Y%m%d-%H%M%S\")))\n",
    "    os.makedirs(model_path)\n",
    "    os.makedirs(os.path.join(model_path, 'weights'))\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        tr_num_correct = 0\n",
    "        tr_num_examples = 0\n",
    "        epoch_combo = 'epoch' + str(epoch)\n",
    "        os.makedirs(os.path.join(model_path, 'weights', epoch_combo))\n",
    "        for inputs, labels in traindataloader:\n",
    "            #labels = labels.unsqueeze(1)\n",
    "            #labels = labels.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(inputs)\n",
    "            #print(predictions.shape)\n",
    "            #print(labels.shape)\n",
    "            loss = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            model_save_dir = str(os.path.join(model_path, 'weights', epoch_combo, 'weights.pt'))\n",
    "            full_path = str(os.getcwd())\n",
    "            #print(model_save_dir)\n",
    "            #torch.save({'epoch': epoch,\n",
    "            #    'model_state_dict': model.state_dict(),\n",
    "            #    'optimizer_state_dict': optimiser.state_dict()},\n",
    "                  #str(os.path.join(full_path, model_save_dir)))\n",
    "            #           model_save_dir)\n",
    "            torch.save({'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimiser.state_dict()},\n",
    "                  str(os.path.join(full_path, model_save_dir)))\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            #batch_idx += 1\n",
    "            training_loss += loss.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(predictions, dim=1), dim=1)[1], labels)\n",
    "            tr_num_correct += torch.sum(correct).item()\n",
    "            tr_num_examples += correct.shape[0]\n",
    "        training_loss /= len(traindataloader.dataset)\n",
    "        training_accuracy = tr_num_correct / tr_num_examples\n",
    "        ## add training performance to tensorboard\n",
    "        writer.add_scalar('Training Loss', training_loss, global_step)\n",
    "        writer.add_scalar('Training Accuracy', training_accuracy, global_step)\n",
    "\n",
    "        model.eval()\n",
    "        val_num_correct = 0\n",
    "        val_num_examples = 0\n",
    "        for inputs, labels in valdataloader:\n",
    "            #labels = labels.unsqueeze(1)\n",
    "            #labels = labels.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(inputs)\n",
    "            loss = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss(predictions, labels)\n",
    "            validation_loss += loss.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(predictions, dim =1), dim=1)[1], labels)\n",
    "            val_num_correct += torch.sum(correct).item()\n",
    "            val_num_examples += correct.shape[0]\n",
    "        validation_loss /= len(valdataloader.dataset)\n",
    "        validation_accuracy = val_num_correct / val_num_examples\n",
    "        ## add validation performance to tensorboard\n",
    "        writer.add_scalar('Validation Loss', validation_loss, global_step)\n",
    "        writer.add_scalar('Validation Accuracy', validation_accuracy, global_step)\n",
    "        perf_dict = {}\n",
    "        perf_dict[epoch] = {'training_loss': training_loss,\n",
    "                            'val_loss': validation_loss,\n",
    "                            'training_accuracy': tr_num_correct / tr_num_examples,\n",
    "                            'val_accuracy': val_num_correct / val_num_examples}\n",
    "\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, train_accuracy = {:.2f},val_accuracy = {:.2f} '.format(epoch, training_loss, validation_loss, tr_num_correct / tr_num_examples,\n",
    "                                                                                                                             val_num_correct / val_num_examples))\n",
    "        global_step += 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "533f1ebd-a4ac-43aa-9bc9-b9c4bd28d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in classifier.resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "## define the layers to unfreeze and then retrain\n",
    "layers_to_unfreeze = ['layers.3']\n",
    "\n",
    "for name, param in classifier.resnet50.named_parameters():\n",
    "    for layer_name in layers_to_unfreeze:\n",
    "        if layer_name in name:\n",
    "            param.requires_grad = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04b76fa1-c60f-44db-ba57-d8d7b2773d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train(model,traindataloader, valdataloader, epochs):\\n    #optimiser = torch.optim.SGD(model.parameters(), lr=0.001)\\n    #optimiser = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\\n    #optimiser = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\\n    optimiser = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9, weight_decay=1e-4)\\n\\n    model_path = str(os.path.join(\\'model_evaluation\\', time.strftime(\"%Y%m%d-%H%M%S\")))\\n    os.makedirs(model_path)\\n    os.makedirs(os.path.join(model_path, \\'weights\\'))\\n\\n    global_step = 0\\n\\n    for epoch in range(epochs):\\n        training_loss = 0.0\\n        validation_loss = 0.0\\n        model.to(device)\\n        model.train()\\n        tr_num_correct = 0\\n        tr_num_examples = 0\\n        epoch_combo = \\'epoch\\' + str(epoch)\\n        os.makedirs(os.path.join(model_path, \\'weights\\', epoch_combo))\\n        for inputs, labels in traindataloader:\\n            #labels = labels.unsqueeze(1)\\n            #labels = labels.float()\\n            inputs = inputs.to(device)\\n            labels = labels.to(device)\\n            predictions = model(inputs)\\n            #print(predictions.shape)\\n            #print(labels.shape)\\n            loss = torch.nn.CrossEntropyLoss()\\n            loss = loss(predictions, labels)\\n            loss.backward()\\n            optimiser.step()\\n            model_save_dir = str(os.path.join(model_path, \\'weights\\', epoch_combo, \\'weights.pt\\'))\\n            full_path = str(os.getcwd())\\n            #print(model_save_dir)\\n            #torch.save({\\'epoch\\': epoch,\\n            #    \\'model_state_dict\\': model.state_dict(),\\n            #    \\'optimizer_state_dict\\': optimiser.state_dict()},\\n                  #str(os.path.join(full_path, model_save_dir)))\\n            #           model_save_dir)\\n            torch.save({\\'epoch\\': epoch,\\n                  \\'model_state_dict\\': model.state_dict(),\\n                  \\'optimizer_state_dict\\': optimiser.state_dict()},\\n                  str(os.path.join(full_path, model_save_dir)))\\n\\n            optimiser.zero_grad()\\n            #batch_idx += 1\\n            training_loss += loss.item() * inputs.size(0)\\n            correct = torch.eq(torch.max(F.softmax(predictions, dim=1), dim=1)[1], labels)\\n            tr_num_correct += torch.sum(correct).item()\\n            tr_num_examples += correct.shape[0]\\n        training_loss /= len(traindataloader.dataset)\\n        training_accuracy = tr_num_correct / tr_num_examples\\n        ## add training performance to tensorboard\\n        writer.add_scalar(\\'Training Loss\\', training_loss, global_step)\\n        writer.add_scalar(\\'Training Accuracy\\', training_accuracy, global_step)\\n\\n        model.eval()\\n        val_num_correct = 0\\n        val_num_examples = 0\\n        for inputs, labels in valdataloader:\\n            #labels = labels.unsqueeze(1)\\n            #labels = labels.float()\\n            inputs = inputs.to(device)\\n            labels = labels.to(device)\\n            predictions = model(inputs)\\n            loss = torch.nn.CrossEntropyLoss()\\n            loss = loss(predictions, labels)\\n            validation_loss += loss.item() * inputs.size(0)\\n            correct = torch.eq(torch.max(F.softmax(predictions, dim =1), dim=1)[1], labels)\\n            val_num_correct += torch.sum(correct).item()\\n            val_num_examples += correct.shape[0]\\n        validation_loss /= len(valdataloader.dataset)\\n        validation_accuracy = val_num_correct / val_num_examples\\n        ## add validation performance to tensorboard\\n        writer.add_scalar(\\'Validation Loss\\', validation_loss, global_step)\\n        writer.add_scalar(\\'Validation Accuracy\\', validation_accuracy, global_step)\\n        perf_dict = {}\\n        perf_dict[epoch] = {\\'training_loss\\': training_loss,\\n                            \\'val_loss\\': validation_loss,\\n                            \\'training_accuracy\\': tr_num_correct / tr_num_examples,\\n                            \\'val_accuracy\\': val_num_correct / val_num_examples}\\n\\n\\n        print(\\'Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, train_accuracy = {:.2f},val_accuracy = {:.2f} \\'.format(epoch, training_loss, validation_loss, tr_num_correct / tr_num_examples,\\n                                                                                                                             val_num_correct / val_num_examples))\\n        global_step += 1\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def train(model,traindataloader, valdataloader, epochs):\n",
    "    #optimiser = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    #optimiser = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    #optimiser = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "    optimiser = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "    model_path = str(os.path.join('model_evaluation', time.strftime(\"%Y%m%d-%H%M%S\")))\n",
    "    os.makedirs(model_path)\n",
    "    os.makedirs(os.path.join(model_path, 'weights'))\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        tr_num_correct = 0\n",
    "        tr_num_examples = 0\n",
    "        epoch_combo = 'epoch' + str(epoch)\n",
    "        os.makedirs(os.path.join(model_path, 'weights', epoch_combo))\n",
    "        for inputs, labels in traindataloader:\n",
    "            #labels = labels.unsqueeze(1)\n",
    "            #labels = labels.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(inputs)\n",
    "            #print(predictions.shape)\n",
    "            #print(labels.shape)\n",
    "            loss = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            model_save_dir = str(os.path.join(model_path, 'weights', epoch_combo, 'weights.pt'))\n",
    "            full_path = str(os.getcwd())\n",
    "            #print(model_save_dir)\n",
    "            #torch.save({'epoch': epoch,\n",
    "            #    'model_state_dict': model.state_dict(),\n",
    "            #    'optimizer_state_dict': optimiser.state_dict()},\n",
    "                  #str(os.path.join(full_path, model_save_dir)))\n",
    "            #           model_save_dir)\n",
    "            torch.save({'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimiser.state_dict()},\n",
    "                  str(os.path.join(full_path, model_save_dir)))\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            #batch_idx += 1\n",
    "            training_loss += loss.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(predictions, dim=1), dim=1)[1], labels)\n",
    "            tr_num_correct += torch.sum(correct).item()\n",
    "            tr_num_examples += correct.shape[0]\n",
    "        training_loss /= len(traindataloader.dataset)\n",
    "        training_accuracy = tr_num_correct / tr_num_examples\n",
    "        ## add training performance to tensorboard\n",
    "        writer.add_scalar('Training Loss', training_loss, global_step)\n",
    "        writer.add_scalar('Training Accuracy', training_accuracy, global_step)\n",
    "\n",
    "        model.eval()\n",
    "        val_num_correct = 0\n",
    "        val_num_examples = 0\n",
    "        for inputs, labels in valdataloader:\n",
    "            #labels = labels.unsqueeze(1)\n",
    "            #labels = labels.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(inputs)\n",
    "            loss = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss(predictions, labels)\n",
    "            validation_loss += loss.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(predictions, dim =1), dim=1)[1], labels)\n",
    "            val_num_correct += torch.sum(correct).item()\n",
    "            val_num_examples += correct.shape[0]\n",
    "        validation_loss /= len(valdataloader.dataset)\n",
    "        validation_accuracy = val_num_correct / val_num_examples\n",
    "        ## add validation performance to tensorboard\n",
    "        writer.add_scalar('Validation Loss', validation_loss, global_step)\n",
    "        writer.add_scalar('Validation Accuracy', validation_accuracy, global_step)\n",
    "        perf_dict = {}\n",
    "        perf_dict[epoch] = {'training_loss': training_loss,\n",
    "                            'val_loss': validation_loss,\n",
    "                            'training_accuracy': tr_num_correct / tr_num_examples,\n",
    "                            'val_accuracy': val_num_correct / val_num_examples}\n",
    "\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, train_accuracy = {:.2f},val_accuracy = {:.2f} '.format(epoch, training_loss, validation_loss, tr_num_correct / tr_num_examples,\n",
    "                                                                                                                             val_num_correct / val_num_examples))\n",
    "        global_step += 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb1dd742-9397-42af-9bf5-431570f36888",
   "metadata": {},
   "outputs": [],
   "source": [
    "## variation in training loop\n",
    "\n",
    "def train(model, traindataloader, valdataloader, epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    writer = SummaryWriter()\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model_path = os.path.join('model_evaluation', time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    os.makedirs(model_path)\n",
    "    os.makedirs(os.path.join(model_path, 'weights'))\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        tr_num_correct = 0\n",
    "        tr_num_examples = 0\n",
    "        epoch_combo = 'epoch' + str(epoch)\n",
    "        os.makedirs(os.path.join(model_path, 'weights', epoch_combo))\n",
    "\n",
    "        model.train()\n",
    "        for inputs, labels in traindataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(inputs)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            training_loss += loss.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(predictions, dim=1), dim=1)[1], labels)\n",
    "            tr_num_correct += torch.sum(correct).item()\n",
    "            tr_num_examples += correct.shape[0]\n",
    "\n",
    "        training_loss /= len(traindataloader.dataset)\n",
    "        training_accuracy = tr_num_correct / tr_num_examples\n",
    "        writer.add_scalar('Training Loss', training_loss, global_step)\n",
    "        writer.add_scalar('Training Accuracy', training_accuracy, global_step)\n",
    "\n",
    "        model.eval()\n",
    "        val_num_correct = 0\n",
    "        val_num_examples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valdataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                predictions = model(inputs)\n",
    "                loss = criterion(predictions, labels)\n",
    "                validation_loss += loss.item() * inputs.size(0)\n",
    "                correct = torch.eq(torch.max(F.softmax(predictions, dim=1), dim=1)[1], labels)\n",
    "                val_num_correct += torch.sum(correct).item()\n",
    "                val_num_examples += correct.shape[0]\n",
    "\n",
    "        validation_loss /= len(valdataloader.dataset)\n",
    "        validation_accuracy = val_num_correct / val_num_examples\n",
    "        writer.add_scalar('Validation Loss', validation_loss, global_step)\n",
    "        writer.add_scalar('Validation Accuracy', validation_accuracy, global_step)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, train_accuracy = {:.2f}, val_accuracy = {:.2f}'.format(epoch, training_loss, validation_loss, training_accuracy, validation_accuracy))\n",
    "\n",
    "        # Save the model checkpoint at the end of each epoch\n",
    "        model_save_dir = os.path.join(model_path, 'weights', epoch_combo, 'weights.pt')\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "                    model_save_dir)\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d2b4b8d-c182-4ab5-9150-825557184273",
   "metadata": {
    "id": "7d2b4b8d-c182-4ab5-9150-825557184273"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## unfreeze last two layers\\nfor param in classifier.resnet50.layer3:\\n  param.requires_grad=True\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## unfreeze last two layers\n",
    "for param in classifier.resnet50.layer3:\n",
    "  param.requires_grad=True\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "088dc544-bbd6-4465-9cb0-84a3c709e28e",
   "metadata": {
    "id": "088dc544-bbd6-4465-9cb0-84a3c709e28e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## define the layers to unfreeze and then retrain\\n\\nlayers_to_unfreeze = ['layers.2', 'layers.3']\\n\\nfor name, param in classifier.resnet50.named_parameters():\\n    for layer_name in layers_to_unfreeze:\\n        if layer_name in name:\\n            param.requires_grad = True\\n            break\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## define the layers to unfreeze and then retrain\n",
    "\n",
    "layers_to_unfreeze = ['layers.2', 'layers.3']\n",
    "\n",
    "for name, param in classifier.resnet50.named_parameters():\n",
    "    for layer_name in layers_to_unfreeze:\n",
    "        if layer_name in name:\n",
    "            param.requires_grad = True\n",
    "            break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855620e-03fd-439e-bd15-5aa9960d2d49",
   "metadata": {
    "id": "e855620e-03fd-439e-bd15-5aa9960d2d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 2.57, Validation Loss: 2.56, train_accuracy = 0.08, val_accuracy = 0.09\n",
      "Epoch: 1, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.09, val_accuracy = 0.08\n",
      "Epoch: 2, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.09, val_accuracy = 0.09\n",
      "Epoch: 3, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.09, val_accuracy = 0.09\n",
      "Epoch: 4, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.09, val_accuracy = 0.09\n",
      "Epoch: 5, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.10, val_accuracy = 0.10\n",
      "Epoch: 6, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.10, val_accuracy = 0.09\n",
      "Epoch: 7, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.10, val_accuracy = 0.09\n",
      "Epoch: 8, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.11, val_accuracy = 0.09\n",
      "Epoch: 9, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.11, val_accuracy = 0.09\n",
      "Epoch: 10, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.11, val_accuracy = 0.09\n",
      "Epoch: 11, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.11, val_accuracy = 0.09\n",
      "Epoch: 12, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.12, val_accuracy = 0.09\n",
      "Epoch: 13, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.12, val_accuracy = 0.09\n",
      "Epoch: 14, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.12, val_accuracy = 0.10\n",
      "Epoch: 15, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.13, val_accuracy = 0.09\n",
      "Epoch: 16, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.13, val_accuracy = 0.10\n",
      "Epoch: 17, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.13, val_accuracy = 0.09\n",
      "Epoch: 18, Training Loss: 2.56, Validation Loss: 2.56, train_accuracy = 0.14, val_accuracy = 0.10\n",
      "Epoch: 19, Training Loss: 2.55, Validation Loss: 2.56, train_accuracy = 0.14, val_accuracy = 0.10\n",
      "Epoch: 20, Training Loss: 2.55, Validation Loss: 2.56, train_accuracy = 0.14, val_accuracy = 0.10\n",
      "Epoch: 21, Training Loss: 2.55, Validation Loss: 2.56, train_accuracy = 0.15, val_accuracy = 0.10\n",
      "Epoch: 22, Training Loss: 2.55, Validation Loss: 2.56, train_accuracy = 0.15, val_accuracy = 0.10\n",
      "Epoch: 23, Training Loss: 2.55, Validation Loss: 2.56, train_accuracy = 0.15, val_accuracy = 0.10\n",
      "Epoch: 24, Training Loss: 2.55, Validation Loss: 2.56, train_accuracy = 0.16, val_accuracy = 0.10\n",
      "Epoch: 25, Training Loss: 2.55, Validation Loss: 2.56, train_accuracy = 0.16, val_accuracy = 0.10\n",
      "Epoch: 26, Training Loss: 2.55, Validation Loss: 2.56, train_accuracy = 0.16, val_accuracy = 0.10\n",
      "Epoch: 27, Training Loss: 2.55, Validation Loss: 2.56, train_accuracy = 0.16, val_accuracy = 0.10\n",
      "Epoch: 28, Training Loss: 2.55, Validation Loss: 2.56, train_accuracy = 0.17, val_accuracy = 0.10\n",
      "Epoch: 29, Training Loss: 2.55, Validation Loss: 2.56, train_accuracy = 0.16, val_accuracy = 0.10\n",
      "Epoch: 30, Training Loss: 2.54, Validation Loss: 2.56, train_accuracy = 0.16, val_accuracy = 0.10\n",
      "Epoch: 31, Training Loss: 2.54, Validation Loss: 2.56, train_accuracy = 0.17, val_accuracy = 0.10\n",
      "Epoch: 32, Training Loss: 2.54, Validation Loss: 2.56, train_accuracy = 0.17, val_accuracy = 0.10\n",
      "Epoch: 33, Training Loss: 2.54, Validation Loss: 2.56, train_accuracy = 0.17, val_accuracy = 0.10\n",
      "Epoch: 34, Training Loss: 2.54, Validation Loss: 2.56, train_accuracy = 0.17, val_accuracy = 0.10\n",
      "Epoch: 35, Training Loss: 2.54, Validation Loss: 2.56, train_accuracy = 0.17, val_accuracy = 0.11\n",
      "Epoch: 36, Training Loss: 2.54, Validation Loss: 2.56, train_accuracy = 0.18, val_accuracy = 0.10\n",
      "Epoch: 37, Training Loss: 2.54, Validation Loss: 2.56, train_accuracy = 0.18, val_accuracy = 0.10\n",
      "Epoch: 38, Training Loss: 2.54, Validation Loss: 2.56, train_accuracy = 0.18, val_accuracy = 0.10\n",
      "Epoch: 39, Training Loss: 2.53, Validation Loss: 2.56, train_accuracy = 0.18, val_accuracy = 0.11\n",
      "Epoch: 40, Training Loss: 2.53, Validation Loss: 2.56, train_accuracy = 0.19, val_accuracy = 0.11\n",
      "Epoch: 41, Training Loss: 2.53, Validation Loss: 2.56, train_accuracy = 0.19, val_accuracy = 0.11\n",
      "Epoch: 42, Training Loss: 2.53, Validation Loss: 2.56, train_accuracy = 0.20, val_accuracy = 0.11\n",
      "Epoch: 43, Training Loss: 2.53, Validation Loss: 2.56, train_accuracy = 0.19, val_accuracy = 0.11\n",
      "Epoch: 44, Training Loss: 2.53, Validation Loss: 2.56, train_accuracy = 0.20, val_accuracy = 0.11\n",
      "Epoch: 45, Training Loss: 2.53, Validation Loss: 2.56, train_accuracy = 0.20, val_accuracy = 0.11\n",
      "Epoch: 46, Training Loss: 2.52, Validation Loss: 2.56, train_accuracy = 0.20, val_accuracy = 0.11\n",
      "Epoch: 47, Training Loss: 2.52, Validation Loss: 2.56, train_accuracy = 0.20, val_accuracy = 0.11\n",
      "Epoch: 48, Training Loss: 2.52, Validation Loss: 2.56, train_accuracy = 0.20, val_accuracy = 0.11\n",
      "Epoch: 49, Training Loss: 2.52, Validation Loss: 2.56, train_accuracy = 0.21, val_accuracy = 0.10\n",
      "Epoch: 50, Training Loss: 2.52, Validation Loss: 2.56, train_accuracy = 0.20, val_accuracy = 0.10\n",
      "Epoch: 51, Training Loss: 2.52, Validation Loss: 2.56, train_accuracy = 0.21, val_accuracy = 0.10\n",
      "Epoch: 52, Training Loss: 2.51, Validation Loss: 2.56, train_accuracy = 0.21, val_accuracy = 0.10\n",
      "Epoch: 53, Training Loss: 2.51, Validation Loss: 2.56, train_accuracy = 0.21, val_accuracy = 0.11\n",
      "Epoch: 54, Training Loss: 2.51, Validation Loss: 2.56, train_accuracy = 0.22, val_accuracy = 0.11\n",
      "Epoch: 55, Training Loss: 2.51, Validation Loss: 2.56, train_accuracy = 0.22, val_accuracy = 0.11\n",
      "Epoch: 56, Training Loss: 2.51, Validation Loss: 2.56, train_accuracy = 0.22, val_accuracy = 0.10\n",
      "Epoch: 57, Training Loss: 2.51, Validation Loss: 2.56, train_accuracy = 0.23, val_accuracy = 0.11\n",
      "Epoch: 58, Training Loss: 2.51, Validation Loss: 2.56, train_accuracy = 0.22, val_accuracy = 0.11\n",
      "Epoch: 59, Training Loss: 2.50, Validation Loss: 2.56, train_accuracy = 0.23, val_accuracy = 0.11\n",
      "Epoch: 60, Training Loss: 2.50, Validation Loss: 2.56, train_accuracy = 0.23, val_accuracy = 0.11\n",
      "Epoch: 61, Training Loss: 2.50, Validation Loss: 2.56, train_accuracy = 0.22, val_accuracy = 0.11\n",
      "Epoch: 62, Training Loss: 2.50, Validation Loss: 2.56, train_accuracy = 0.24, val_accuracy = 0.10\n",
      "Epoch: 63, Training Loss: 2.50, Validation Loss: 2.56, train_accuracy = 0.24, val_accuracy = 0.10\n",
      "Epoch: 64, Training Loss: 2.50, Validation Loss: 2.56, train_accuracy = 0.24, val_accuracy = 0.10\n",
      "Epoch: 65, Training Loss: 2.49, Validation Loss: 2.56, train_accuracy = 0.24, val_accuracy = 0.10\n",
      "Epoch: 66, Training Loss: 2.49, Validation Loss: 2.56, train_accuracy = 0.24, val_accuracy = 0.11\n",
      "Epoch: 67, Training Loss: 2.49, Validation Loss: 2.56, train_accuracy = 0.25, val_accuracy = 0.11\n",
      "Epoch: 68, Training Loss: 2.49, Validation Loss: 2.56, train_accuracy = 0.25, val_accuracy = 0.11\n",
      "Epoch: 69, Training Loss: 2.49, Validation Loss: 2.56, train_accuracy = 0.25, val_accuracy = 0.11\n",
      "Epoch: 70, Training Loss: 2.48, Validation Loss: 2.56, train_accuracy = 0.26, val_accuracy = 0.11\n",
      "Epoch: 71, Training Loss: 2.48, Validation Loss: 2.56, train_accuracy = 0.25, val_accuracy = 0.11\n",
      "Epoch: 72, Training Loss: 2.48, Validation Loss: 2.56, train_accuracy = 0.25, val_accuracy = 0.11\n",
      "Epoch: 73, Training Loss: 2.48, Validation Loss: 2.56, train_accuracy = 0.26, val_accuracy = 0.11\n",
      "Epoch: 74, Training Loss: 2.48, Validation Loss: 2.56, train_accuracy = 0.26, val_accuracy = 0.11\n",
      "Epoch: 75, Training Loss: 2.47, Validation Loss: 2.56, train_accuracy = 0.27, val_accuracy = 0.11\n",
      "Epoch: 76, Training Loss: 2.47, Validation Loss: 2.56, train_accuracy = 0.27, val_accuracy = 0.11\n",
      "Epoch: 77, Training Loss: 2.47, Validation Loss: 2.56, train_accuracy = 0.26, val_accuracy = 0.11\n",
      "Epoch: 78, Training Loss: 2.47, Validation Loss: 2.56, train_accuracy = 0.27, val_accuracy = 0.11\n",
      "Epoch: 79, Training Loss: 2.47, Validation Loss: 2.55, train_accuracy = 0.27, val_accuracy = 0.11\n",
      "Epoch: 80, Training Loss: 2.47, Validation Loss: 2.55, train_accuracy = 0.27, val_accuracy = 0.11\n",
      "Epoch: 81, Training Loss: 2.46, Validation Loss: 2.56, train_accuracy = 0.27, val_accuracy = 0.11\n",
      "Epoch: 82, Training Loss: 2.46, Validation Loss: 2.55, train_accuracy = 0.27, val_accuracy = 0.11\n",
      "Epoch: 83, Training Loss: 2.46, Validation Loss: 2.55, train_accuracy = 0.28, val_accuracy = 0.11\n",
      "Epoch: 84, Training Loss: 2.46, Validation Loss: 2.55, train_accuracy = 0.28, val_accuracy = 0.11\n",
      "Epoch: 85, Training Loss: 2.46, Validation Loss: 2.55, train_accuracy = 0.28, val_accuracy = 0.11\n",
      "Epoch: 86, Training Loss: 2.46, Validation Loss: 2.55, train_accuracy = 0.28, val_accuracy = 0.11\n",
      "Epoch: 87, Training Loss: 2.45, Validation Loss: 2.55, train_accuracy = 0.29, val_accuracy = 0.11\n",
      "Epoch: 88, Training Loss: 2.45, Validation Loss: 2.55, train_accuracy = 0.29, val_accuracy = 0.11\n",
      "Epoch: 89, Training Loss: 2.45, Validation Loss: 2.55, train_accuracy = 0.29, val_accuracy = 0.11\n",
      "Epoch: 90, Training Loss: 2.45, Validation Loss: 2.55, train_accuracy = 0.29, val_accuracy = 0.11\n",
      "Epoch: 91, Training Loss: 2.45, Validation Loss: 2.55, train_accuracy = 0.29, val_accuracy = 0.11\n",
      "Epoch: 92, Training Loss: 2.45, Validation Loss: 2.55, train_accuracy = 0.29, val_accuracy = 0.11\n",
      "Epoch: 93, Training Loss: 2.44, Validation Loss: 2.55, train_accuracy = 0.30, val_accuracy = 0.11\n",
      "Epoch: 94, Training Loss: 2.44, Validation Loss: 2.55, train_accuracy = 0.30, val_accuracy = 0.11\n",
      "Epoch: 95, Training Loss: 2.44, Validation Loss: 2.55, train_accuracy = 0.30, val_accuracy = 0.12\n",
      "Epoch: 96, Training Loss: 2.44, Validation Loss: 2.55, train_accuracy = 0.31, val_accuracy = 0.12\n",
      "Epoch: 97, Training Loss: 2.44, Validation Loss: 2.55, train_accuracy = 0.30, val_accuracy = 0.12\n",
      "Epoch: 98, Training Loss: 2.44, Validation Loss: 2.55, train_accuracy = 0.30, val_accuracy = 0.11\n",
      "Epoch: 99, Training Loss: 2.43, Validation Loss: 2.55, train_accuracy = 0.30, val_accuracy = 0.11\n",
      "Epoch: 100, Training Loss: 2.43, Validation Loss: 2.55, train_accuracy = 0.30, val_accuracy = 0.12\n",
      "Epoch: 101, Training Loss: 2.43, Validation Loss: 2.55, train_accuracy = 0.30, val_accuracy = 0.12\n",
      "Epoch: 102, Training Loss: 2.43, Validation Loss: 2.55, train_accuracy = 0.30, val_accuracy = 0.12\n",
      "Epoch: 103, Training Loss: 2.43, Validation Loss: 2.55, train_accuracy = 0.31, val_accuracy = 0.12\n",
      "Epoch: 104, Training Loss: 2.43, Validation Loss: 2.55, train_accuracy = 0.32, val_accuracy = 0.12\n",
      "Epoch: 105, Training Loss: 2.43, Validation Loss: 2.55, train_accuracy = 0.31, val_accuracy = 0.11\n",
      "Epoch: 106, Training Loss: 2.42, Validation Loss: 2.55, train_accuracy = 0.31, val_accuracy = 0.12\n",
      "Epoch: 107, Training Loss: 2.42, Validation Loss: 2.55, train_accuracy = 0.32, val_accuracy = 0.12\n",
      "Epoch: 108, Training Loss: 2.42, Validation Loss: 2.55, train_accuracy = 0.32, val_accuracy = 0.12\n",
      "Epoch: 109, Training Loss: 2.42, Validation Loss: 2.55, train_accuracy = 0.32, val_accuracy = 0.12\n",
      "Epoch: 110, Training Loss: 2.42, Validation Loss: 2.55, train_accuracy = 0.32, val_accuracy = 0.12\n",
      "Epoch: 111, Training Loss: 2.42, Validation Loss: 2.55, train_accuracy = 0.32, val_accuracy = 0.12\n",
      "Epoch: 112, Training Loss: 2.41, Validation Loss: 2.55, train_accuracy = 0.32, val_accuracy = 0.12\n",
      "Epoch: 113, Training Loss: 2.41, Validation Loss: 2.55, train_accuracy = 0.32, val_accuracy = 0.12\n",
      "Epoch: 114, Training Loss: 2.41, Validation Loss: 2.55, train_accuracy = 0.33, val_accuracy = 0.13\n",
      "Epoch: 115, Training Loss: 2.41, Validation Loss: 2.55, train_accuracy = 0.33, val_accuracy = 0.12\n",
      "Epoch: 116, Training Loss: 2.41, Validation Loss: 2.55, train_accuracy = 0.33, val_accuracy = 0.12\n",
      "Epoch: 117, Training Loss: 2.40, Validation Loss: 2.55, train_accuracy = 0.33, val_accuracy = 0.12\n",
      "Epoch: 118, Training Loss: 2.40, Validation Loss: 2.55, train_accuracy = 0.33, val_accuracy = 0.12\n",
      "Epoch: 119, Training Loss: 2.40, Validation Loss: 2.55, train_accuracy = 0.33, val_accuracy = 0.12\n",
      "Epoch: 120, Training Loss: 2.40, Validation Loss: 2.55, train_accuracy = 0.33, val_accuracy = 0.13\n",
      "Epoch: 121, Training Loss: 2.40, Validation Loss: 2.55, train_accuracy = 0.33, val_accuracy = 0.12\n",
      "Epoch: 122, Training Loss: 2.40, Validation Loss: 2.55, train_accuracy = 0.34, val_accuracy = 0.12\n",
      "Epoch: 123, Training Loss: 2.40, Validation Loss: 2.55, train_accuracy = 0.34, val_accuracy = 0.12\n",
      "Epoch: 124, Training Loss: 2.40, Validation Loss: 2.55, train_accuracy = 0.34, val_accuracy = 0.12\n",
      "Epoch: 125, Training Loss: 2.39, Validation Loss: 2.55, train_accuracy = 0.34, val_accuracy = 0.13\n",
      "Epoch: 126, Training Loss: 2.39, Validation Loss: 2.55, train_accuracy = 0.34, val_accuracy = 0.12\n",
      "Epoch: 127, Training Loss: 2.39, Validation Loss: 2.55, train_accuracy = 0.35, val_accuracy = 0.13\n",
      "Epoch: 128, Training Loss: 2.39, Validation Loss: 2.55, train_accuracy = 0.35, val_accuracy = 0.12\n",
      "Epoch: 129, Training Loss: 2.39, Validation Loss: 2.55, train_accuracy = 0.35, val_accuracy = 0.13\n",
      "Epoch: 130, Training Loss: 2.39, Validation Loss: 2.55, train_accuracy = 0.34, val_accuracy = 0.13\n",
      "Epoch: 131, Training Loss: 2.39, Validation Loss: 2.55, train_accuracy = 0.35, val_accuracy = 0.12\n",
      "Epoch: 132, Training Loss: 2.38, Validation Loss: 2.55, train_accuracy = 0.35, val_accuracy = 0.13\n",
      "Epoch: 133, Training Loss: 2.38, Validation Loss: 2.55, train_accuracy = 0.35, val_accuracy = 0.13\n",
      "Epoch: 134, Training Loss: 2.38, Validation Loss: 2.55, train_accuracy = 0.36, val_accuracy = 0.13\n",
      "Epoch: 135, Training Loss: 2.38, Validation Loss: 2.55, train_accuracy = 0.36, val_accuracy = 0.13\n",
      "Epoch: 136, Training Loss: 2.38, Validation Loss: 2.55, train_accuracy = 0.36, val_accuracy = 0.13\n",
      "Epoch: 137, Training Loss: 2.38, Validation Loss: 2.55, train_accuracy = 0.36, val_accuracy = 0.13\n",
      "Epoch: 138, Training Loss: 2.37, Validation Loss: 2.55, train_accuracy = 0.36, val_accuracy = 0.13\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = ItemsTrainDataSet()\n",
    "#val_dataset = ItemsValDataSet()\n",
    "train_loader = DataLoader(dataset = traindataset, batch_size=32)\n",
    "val_loader = DataLoader(dataset = valdataset, batch_size=32)\n",
    "train(classifier, traindataloader= train_loader, valdataloader= val_loader, epochs=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LbaszzbFrKeE",
   "metadata": {
    "id": "LbaszzbFrKeE"
   },
   "outputs": [],
   "source": [
    "#atch size 32 - slow but good"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
