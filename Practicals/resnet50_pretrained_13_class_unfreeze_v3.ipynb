{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8def34-fade-43cc-b2c5-605a8bc5c857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import storage\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "# %load_ext tensorboard\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from torchvision.models.resnet import *\n",
    "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25744888",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6010f360-bb71-4b90-8ba8-d1d074259cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/facebook-marketplaces-recommendation-ranking-system/Practicals'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9734df-f22b-460a-9c87-142a45123c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a train dataset class\n",
    "class ItemsTrainDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.examples = self._load_examples()\n",
    "        self.pil_to_tensor = transforms.ToTensor()\n",
    "        self.resize = transforms.Resize((225,225))\n",
    "        #self.rgbify = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n",
    "\n",
    "    def _load_examples(self):\n",
    "        class_names = os.listdir('pytorch_images_tv_split_2/train')\n",
    "        class_encoder = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "        class_decoder = {idx: class_name for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "        examples_list = []\n",
    "        for cl_name in class_names:\n",
    "            example_fp = os.listdir(os.path.join('pytorch_images_tv_split_2/train',cl_name))\n",
    "            example_fp = [os.path.join('pytorch_images_tv_split_2/train', cl_name, img_name ) for img_name in example_fp]\n",
    "            example = [(img_name, class_encoder[cl_name]) for img_name in example_fp]\n",
    "            examples_list.extend(example)\n",
    "\n",
    "        return examples_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_fp, img_class = self.examples[idx]\n",
    "        img = Image.open(img_fp)\n",
    "\n",
    "        features = self.pil_to_tensor(img)\n",
    "        features = self.resize(features)\n",
    "        #features = self.rgbify(features)\n",
    "\n",
    "        return features, img_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "335d1a2c-34e5-4d3a-9be1-2706e4cb014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creates a validation dataset class\n",
    "class ItemsValDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.examples = self._load_examples()\n",
    "        self.pil_to_tensor = transforms.ToTensor()\n",
    "        self.resize = transforms.Resize((225,225))\n",
    "        #self.rgbify = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)!=1 else x)\n",
    "\n",
    "    def _load_examples(self):\n",
    "        class_names = os.listdir('pytorch_images_tv_split_2/val')\n",
    "        class_encoder = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "        class_decoder = {idx: class_name for idx, class_name in enumerate(class_names)}\n",
    "        examples_list = []\n",
    "        \n",
    "        for cl_name in class_names:\n",
    "            example_fp = os.listdir(os.path.join('pytorch_images_tv_split_2/val',cl_name))\n",
    "            example_fp = [os.path.join('pytorch_images_tv_split_2/val', cl_name, img_name ) for img_name in example_fp]\n",
    "            example = [(img_name, class_encoder[cl_name]) for img_name in example_fp]\n",
    "            examples_list.extend(example)\n",
    "\n",
    "        return examples_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_fp, img_class = self.examples[idx]\n",
    "        img = Image.open(img_fp)\n",
    "\n",
    "        features = self.pil_to_tensor(img)\n",
    "        features = self.resize(features)\n",
    "        #features = self.rgbify(features)\n",
    "\n",
    "        return features, img_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5598f647-b593-4dd2-acd8-7040c6ec2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = ItemsTrainDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d3b971-a200-4410-aa28-ba442b840b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2132"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5754c01e-76ca-452a-8110-6f3b2aa8f9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valdataset = ItemsValDataSet()\n",
    "len(valdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "060b5233-c30a-4fbe-aa11-17ce9f3e7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Created a classifier based on the RESNET50 pretrained model\n",
    "\n",
    "class ItemClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
    "        #self.resnet50 = model\n",
    "        self.resnet50.fc = torch.nn.Linear(2048,13)\n",
    "  \n",
    "    def forward(self, X):\n",
    "        return F.softmax(self.resnet50(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd79aea-afa0-4142-b5c5-8e141a7a223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,traindataloader, valdataloader, epochs):\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    model_path = str(os.path.join('model_evaluation', time.strftime(\"%Y%m%d-%H%M%S\")))   \n",
    "    os.makedirs(model_path)\n",
    "    os.makedirs(os.path.join(model_path, 'weights'))\n",
    "    batch_idx = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        tr_num_correct = 0\n",
    "        tr_num_examples = 0\n",
    "        epoch_combo = 'epoch' + str(epoch)\n",
    "        os.makedirs(os.path.join(model_path, 'weights', epoch_combo))\n",
    "        for inputs, labels in traindataloader:\n",
    "            #labels = labels.unsqueeze(1)\n",
    "            #labels = labels.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(inputs)\n",
    "            #print(predictions.shape)\n",
    "            #print(labels.shape)\n",
    "            loss = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            model_save_dir = str(os.path.join(model_path, 'weights', epoch_combo, 'weights.pth'))\n",
    "            full_path = str('/home/jupyter/facebook-marketplaces-recommendation-ranking-system/Practicals')\n",
    "            #print(model_save_dir)\n",
    "            #torch.save({'epoch': epoch,\n",
    "            #    'model_state_dict': model.state_dict(),\n",
    "            #    'optimizer_state_dict': optimiser.state_dict()}, \n",
    "                  #str(os.path.join(full_path, model_save_dir)))\n",
    "            #           model_save_dir)\n",
    "            torch.save({'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimiser.state_dict()},\n",
    "                  str(os.path.join(full_path, model_save_dir)))\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            batch_idx += 1\n",
    "            training_loss += loss.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(predictions, dim=1), dim=1)[1], labels)\n",
    "            tr_num_correct += torch.sum(correct).item()\n",
    "            tr_num_examples += correct.shape[0]\n",
    "        training_loss /= len(traindataloader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_num_correct = 0\n",
    "        val_num_examples = 0\n",
    "        for inputs, labels in valdataloader:\n",
    "            #labels = labels.unsqueeze(1)\n",
    "            #labels = labels.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(inputs)\n",
    "            loss = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss(predictions, labels)\n",
    "            validation_loss += loss.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(predictions, dim =1), dim=1)[1], labels)\n",
    "            val_num_correct += torch.sum(correct).item()\n",
    "            val_num_examples += correct.shape[0]\n",
    "        validation_loss /= len(valdataloader.dataset)\n",
    "        perf_dict = {}\n",
    "        perf_dict[epoch] = {'training_loss': training_loss,\n",
    "                            'val_loss': validation_loss,\n",
    "                            'training_accuracy': tr_num_correct / tr_num_examples,\n",
    "                            'val_accuracy': val_num_correct / val_num_examples}\n",
    "                            \n",
    "                            \n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, train_accuracy = {:.2f},val_accuracy = {:.2f} '.format(epoch, training_loss, validation_loss, tr_num_correct / tr_num_examples,\n",
    "                                                                                                                             val_num_correct / val_num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de356da2-5cf0-4998-9293-b7b218379543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d91ba037-56f2-4878-86d3-14033352b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jupyter/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "classifier = ItemClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092b07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d2b4b8d-c182-4ab5-9150-825557184273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## unfreeze last two layers\\nfor param in classifier.resnet50.layer3:\\n  param.requires_grad=True\\n\\nfor param in classifier.resnet50.layer4:\\n  param.requires_grad=True\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## unfreeze last two layers\n",
    "for param in classifier.resnet50.layer3:\n",
    "  param.requires_grad=True\n",
    "\n",
    "for param in classifier.resnet50.layer4:\n",
    "  param.requires_grad=True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "088dc544-bbd6-4465-9cb0-84a3c709e28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## define the layers to unfreeze and then retrain\\nlayers_to_unfreeze = ['layers.2', 'layers.3']\\n\\nfor name, param in classifier.resnet50.named_parameters():\\n    for layer_name in layers_to_unfreeze:\\n        if layer_name in name:\\n            param.requires_grad = True\\n            break\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## define the layers to unfreeze and then retrain\n",
    "layers_to_unfreeze = ['layers.2', 'layers.3']\n",
    "\n",
    "for name, param in classifier.resnet50.named_parameters():\n",
    "    for layer_name in layers_to_unfreeze:\n",
    "        if layer_name in name:\n",
    "            param.requires_grad = True\n",
    "            break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855620e-03fd-439e-bd15-5aa9960d2d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 2.57, Validation Loss: 2.56, train_accuracy = 0.10,val_accuracy = 0.12 \n",
      "Epoch: 2, Training Loss: 2.55, Validation Loss: 2.55, train_accuracy = 0.12,val_accuracy = 0.13 \n",
      "Epoch: 3, Training Loss: 2.53, Validation Loss: 2.55, train_accuracy = 0.14,val_accuracy = 0.13 \n",
      "Epoch: 4, Training Loss: 2.52, Validation Loss: 2.54, train_accuracy = 0.15,val_accuracy = 0.13 \n",
      "Epoch: 5, Training Loss: 2.50, Validation Loss: 2.54, train_accuracy = 0.16,val_accuracy = 0.13 \n",
      "Epoch: 6, Training Loss: 2.49, Validation Loss: 2.54, train_accuracy = 0.18,val_accuracy = 0.12 \n",
      "Epoch: 7, Training Loss: 2.48, Validation Loss: 2.53, train_accuracy = 0.19,val_accuracy = 0.12 \n",
      "Epoch: 8, Training Loss: 2.47, Validation Loss: 2.53, train_accuracy = 0.19,val_accuracy = 0.12 \n",
      "Epoch: 9, Training Loss: 2.45, Validation Loss: 2.53, train_accuracy = 0.20,val_accuracy = 0.12 \n",
      "Epoch: 11, Training Loss: 2.43, Validation Loss: 2.53, train_accuracy = 0.22,val_accuracy = 0.13 \n",
      "Epoch: 12, Training Loss: 2.43, Validation Loss: 2.52, train_accuracy = 0.23,val_accuracy = 0.14 \n",
      "Epoch: 13, Training Loss: 2.42, Validation Loss: 2.52, train_accuracy = 0.25,val_accuracy = 0.17 \n",
      "Epoch: 14, Training Loss: 2.41, Validation Loss: 2.52, train_accuracy = 0.26,val_accuracy = 0.20 \n",
      "Epoch: 15, Training Loss: 2.40, Validation Loss: 2.51, train_accuracy = 0.27,val_accuracy = 0.22 \n",
      "Epoch: 17, Training Loss: 2.38, Validation Loss: 2.50, train_accuracy = 0.30,val_accuracy = 0.23 \n",
      "Epoch: 18, Training Loss: 2.38, Validation Loss: 2.50, train_accuracy = 0.31,val_accuracy = 0.22 \n",
      "Epoch: 19, Training Loss: 2.37, Validation Loss: 2.49, train_accuracy = 0.33,val_accuracy = 0.22 \n",
      "Epoch: 20, Training Loss: 2.36, Validation Loss: 2.49, train_accuracy = 0.33,val_accuracy = 0.21 \n",
      "Epoch: 21, Training Loss: 2.35, Validation Loss: 2.49, train_accuracy = 0.34,val_accuracy = 0.21 \n",
      "Epoch: 22, Training Loss: 2.34, Validation Loss: 2.49, train_accuracy = 0.35,val_accuracy = 0.21 \n",
      "Epoch: 23, Training Loss: 2.33, Validation Loss: 2.48, train_accuracy = 0.35,val_accuracy = 0.21 \n",
      "Epoch: 24, Training Loss: 2.33, Validation Loss: 2.48, train_accuracy = 0.36,val_accuracy = 0.22 \n",
      "Epoch: 26, Training Loss: 2.31, Validation Loss: 2.48, train_accuracy = 0.37,val_accuracy = 0.23 \n",
      "Epoch: 27, Training Loss: 2.30, Validation Loss: 2.48, train_accuracy = 0.39,val_accuracy = 0.25 \n",
      "Epoch: 28, Training Loss: 2.29, Validation Loss: 2.48, train_accuracy = 0.39,val_accuracy = 0.26 \n",
      "Epoch: 29, Training Loss: 2.29, Validation Loss: 2.47, train_accuracy = 0.40,val_accuracy = 0.26 \n",
      "Epoch: 30, Training Loss: 2.31, Validation Loss: 2.46, train_accuracy = 0.39,val_accuracy = 0.27 \n",
      "Epoch: 31, Training Loss: 2.30, Validation Loss: 2.46, train_accuracy = 0.40,val_accuracy = 0.27 \n",
      "Epoch: 32, Training Loss: 2.30, Validation Loss: 2.45, train_accuracy = 0.39,val_accuracy = 0.30 \n",
      "Epoch: 33, Training Loss: 2.27, Validation Loss: 2.45, train_accuracy = 0.42,val_accuracy = 0.30 \n",
      "Epoch: 34, Training Loss: 2.26, Validation Loss: 2.45, train_accuracy = 0.43,val_accuracy = 0.30 \n",
      "Epoch: 35, Training Loss: 2.24, Validation Loss: 2.45, train_accuracy = 0.44,val_accuracy = 0.30 \n",
      "Epoch: 36, Training Loss: 2.23, Validation Loss: 2.45, train_accuracy = 0.44,val_accuracy = 0.31 \n",
      "Epoch: 38, Training Loss: 2.22, Validation Loss: 2.45, train_accuracy = 0.46,val_accuracy = 0.31 \n",
      "Epoch: 39, Training Loss: 2.21, Validation Loss: 2.45, train_accuracy = 0.48,val_accuracy = 0.32 \n",
      "Epoch: 40, Training Loss: 2.20, Validation Loss: 2.45, train_accuracy = 0.49,val_accuracy = 0.32 \n",
      "Epoch: 41, Training Loss: 2.19, Validation Loss: 2.44, train_accuracy = 0.49,val_accuracy = 0.32 \n",
      "Epoch: 42, Training Loss: 2.18, Validation Loss: 2.44, train_accuracy = 0.50,val_accuracy = 0.34 \n",
      "Epoch: 43, Training Loss: 2.18, Validation Loss: 2.44, train_accuracy = 0.51,val_accuracy = 0.36 \n",
      "Epoch: 45, Training Loss: 2.16, Validation Loss: 2.43, train_accuracy = 0.53,val_accuracy = 0.39 \n",
      "Epoch: 46, Training Loss: 2.16, Validation Loss: 2.43, train_accuracy = 0.55,val_accuracy = 0.40 \n",
      "Epoch: 48, Training Loss: 2.14, Validation Loss: 2.42, train_accuracy = 0.59,val_accuracy = 0.40 \n"
     ]
    }
   ],
   "source": [
    "#train_dataset = ItemsTrainDataSet()\n",
    "#val_dataset = ItemsValDataSet()\n",
    "train_loader = DataLoader(dataset = traindataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset = valdataset, batch_size=16)\n",
    "train(classifier, traindataloader= train_loader, valdataloader= val_loader, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f0ccd-4b39-4900-819e-a2d40c28b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb7dd1d-6071-4b18-b684-5cba8389cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b4d33-30ed-411b-a170-c7553faf4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df668687-467b-4c59-ad7e-a30553eed766",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(valdataset)):\n",
    "    print(valdataset[i][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43ea5a-1963-405d-b64a-6edd04e3cc51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
